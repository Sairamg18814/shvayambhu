"""Test BLT multilingual capabilities."""

import sys
sys.path.append('.')

import mlx.core as mx
from core.blt.patching import BLTInputProcessor
from core.blt.entropy import calculate_byte_entropy
import numpy as np

def test_multilingual_support():
    """Test BLT with multiple languages."""
    print("=== BLT Multilingual Test ===")
    
    # Test languages
    language_samples = {
        "English": "The quick brown fox jumps over the lazy dog.",
        "Chinese": "å¿«é€Ÿçš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’ç‹—ã€‚",
        "Arabic": "Ø§Ù„Ø«Ø¹Ù„Ø¨ Ø§Ù„Ø¨Ù†ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹ ÙŠÙ‚ÙØ² ÙÙˆÙ‚ Ø§Ù„ÙƒÙ„Ø¨ Ø§Ù„ÙƒØ³ÙˆÙ„.",
        "Russian": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ ĞºĞ¾Ñ€Ğ¸Ñ‡Ğ½ĞµĞ²Ğ°Ñ Ğ»Ğ¸ÑĞ° Ğ¿Ñ€Ñ‹Ğ³Ğ°ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ»ĞµĞ½Ğ¸Ğ²ÑƒÑ ÑĞ¾Ğ±Ğ°ĞºÑƒ.",
        "Japanese": "é€Ÿã„èŒ¶è‰²ã®ã‚­ãƒ„ãƒãŒæ€ ã‘è€…ã®çŠ¬ã‚’é£›ã³è¶Šãˆã‚‹ã€‚",
        "Korean": "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ëŠ”ë‹¤.",
        "Hindi": "à¤¤à¥‡à¤œà¤¼ à¤­à¥‚à¤°à¥€ à¤²à¥‹à¤®à¤¡à¤¼à¥€ à¤†à¤²à¤¸à¥€ à¤•à¥à¤¤à¥à¤¤à¥‡ à¤•à¥‡ à¤Šà¤ªà¤° à¤•à¥‚à¤¦à¤¤à¥€ à¤¹à¥ˆà¥¤",
        "Hebrew": "×”×©×•×¢×œ ×”×—×•× ×”××”×™×¨ ×§×•×¤×¥ ××¢×œ ×”×›×œ×‘ ×”×¢×¦×œ×Ÿ.",
        "Thai": "à¸ªà¸¸à¸™à¸±à¸‚à¸ˆà¸´à¹‰à¸‡à¸ˆà¸­à¸à¸ªà¸µà¸™à¹‰à¸³à¸•à¸²à¸¥à¸—à¸µà¹ˆà¸£à¸§à¸”à¹€à¸£à¹‡à¸§à¸à¸£à¸°à¹‚à¸”à¸”à¸‚à¹‰à¸²à¸¡à¸ªà¸¸à¸™à¸±à¸‚à¸‚à¸µà¹‰à¹€à¸à¸µà¸¢à¸ˆ",
        "Greek": "Î— Î³ÏÎ®Î³Î¿ÏÎ· ÎºÎ±Ï†Î­ Î±Î»ÎµÏ€Î¿Ï Ï€Î·Î´Î¬ Ï€Î¬Î½Ï‰ Î±Ï€ÏŒ Ï„Î¿ Ï„ÎµÎ¼Ï€Î­Î»Î¹ÎºÎ¿ ÏƒÎºÏ…Î»Î¯.",
        "Emoji": "ğŸ¦ŠğŸƒâ€â™‚ï¸ â¡ï¸ ğŸ¦®ğŸ˜´",
        "Mixed": "Hello ä¸–ç•Œ! Ù…Ø±Ø­Ø¨Ø§ Ğ¼Ğ¸Ñ€! ğŸŒ"
    }
    
    # Initialize processor
    processor = BLTInputProcessor(
        min_patch_size=4,
        max_patch_size=32,
        embedding_dim=768
    )
    
    print("\nTesting language processing and entropy:")
    print("-" * 60)
    
    results = []
    
    for language, text in language_samples.items():
        # Get byte representation
        byte_seq = text.encode('utf-8')
        byte_count = len(byte_seq)
        char_count = len(text)
        
        # Calculate entropy
        entropy = calculate_byte_entropy(byte_seq)
        
        # Process with BLT
        try:
            embeddings, metadata = processor.process_input(text)
            patches = metadata['patches']
            boundaries = metadata['boundaries']
            patch_count = len(patches)
            
            # Calculate average patch size
            patch_lengths = [end - start for start, end in boundaries]
            avg_patch_size = np.mean(patch_lengths) if patch_lengths else 0
            
            status = "âœ… PASS"
            
        except Exception as e:
            status = f"âŒ FAIL: {str(e)[:30]}..."
            patch_count = 0
            avg_patch_size = 0
            
        results.append({
            'language': language,
            'chars': char_count,
            'bytes': byte_count,
            'bytes_per_char': byte_count / char_count,
            'entropy': entropy,
            'patches': patch_count,
            'avg_patch_size': avg_patch_size,
            'status': status
        })
        
        print(f"{language:15} | Chars: {char_count:3} | Bytes: {byte_count:3} | "
              f"B/C: {byte_count/char_count:.1f} | Entropy: {entropy:.2f} | "
              f"Patches: {patch_count:2} | Avg size: {avg_patch_size:.1f} | {status}")
    
    # Test special cases
    print("\n\nTesting special Unicode cases:")
    print("-" * 60)
    
    special_cases = {
        "Zero-width space": "Hello\u200bWorld",
        "Combining chars": "Ã© = e\u0301",
        "RTL-LTR mix": "Hello ×©×œ×•× World",
        "Surrogate pairs": "ğ‡ğğ¥ğ¥ğ¨ ğ•ğ• ğ•£ğ•ğ••",
        "Emoji ZWJ": "ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦",
    }
    
    for case_name, text in special_cases.items():
        byte_seq = text.encode('utf-8')
        try:
            embeddings, metadata = processor.process_input(text)
            print(f"{case_name:20} | {len(byte_seq):3} bytes | âœ… PASS")
        except Exception as e:
            print(f"{case_name:20} | {len(byte_seq):3} bytes | âŒ FAIL: {str(e)[:30]}...")
    
    # Summary
    print("\n\nSummary:")
    print("-" * 60)
    passed = sum(1 for r in results if "PASS" in r['status'])
    total = len(results)
    print(f"Languages tested: {total}")
    print(f"Passed: {passed}/{total} ({passed/total*100:.0f}%)")
    
    # Entropy analysis
    entropies = [r['entropy'] for r in results]
    print(f"\nEntropy statistics:")
    print(f"  Min: {min(entropies):.2f}")
    print(f"  Max: {max(entropies):.2f}")
    print(f"  Mean: {np.mean(entropies):.2f}")
    print(f"  Std: {np.std(entropies):.2f}")
    
    # Patch size analysis by script
    script_stats = {}
    scripts = {
        "Latin": ["English", "Mixed"],
        "CJK": ["Chinese", "Japanese", "Korean"],
        "Arabic": ["Arabic"],
        "Cyrillic": ["Russian"],
        "Indic": ["Hindi", "Thai"],
        "Other": ["Hebrew", "Greek", "Emoji"]
    }
    
    for script_type, langs in scripts.items():
        script_results = [r for r in results if r['language'] in langs and "PASS" in r['status']]
        if script_results:
            avg_patch = np.mean([r['avg_patch_size'] for r in script_results])
            avg_entropy = np.mean([r['entropy'] for r in script_results])
            script_stats[script_type] = (avg_patch, avg_entropy)
    
    print("\nPatch size by script type:")
    for script, (patch_size, entropy) in script_stats.items():
        print(f"  {script:10} | Avg patch: {patch_size:4.1f} bytes | Avg entropy: {entropy:.2f}")
    
    return passed == total

if __name__ == "__main__":
    success = test_multilingual_support()
    
    if success:
        print("\nâœ… All multilingual tests passed!")
    else:
        print("\nâŒ Some tests failed!")
        
    print("\nBLT successfully handles multiple languages, scripts, and Unicode edge cases")
    print("without requiring tokenization or vocabulary!")